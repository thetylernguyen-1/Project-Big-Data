{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e750c9ed",
   "metadata": {},
   "source": [
    "# PROJECT BIG DATA: GROUP 19: CLIMATE CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52789a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302850ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all csv into df\n",
    "df = pd.read_csv(\"data/owid-co2-data.csv\")\n",
    "# df = df[(df['year']>=2000 )&(df['year']<=2019 ) ]\n",
    "df = df[(df['year']>=2000)]\n",
    "df.shape\n",
    "# external resource\n",
    "energy = pd.read_csv(\"data/owid-energy-data.csv\")\n",
    "energy.shape\n",
    "# energy = energy [(energy['year']>= 2000)& (energy['year']<=2019)]\n",
    "df_country= pd.read_csv(\"data/GlobalLandTemperaturesByCountry.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4440065",
   "metadata": {},
   "source": [
    "## 📑 Table of Contents\n",
    "\n",
    "- [EDA](#eda)\n",
    "  - [Data wrangling:](#data-wrangling)\n",
    "    - [Quality checks:](#quality-checks)\n",
    "    - [Clean the data](#clean-the-data)\n",
    "- [Machine Learning models: For 3 main research questions:](#machine-learning-modaels-for-3-main-research-questions)\n",
    "  - [ML: RQ: How accurately can a machine-learning model predict a country’s per-capita CO₂ emissions…?](#ml-rq-how-accurately-can-a-machine-learning-model-predict-a-countrys-per-capita-co₂-emissions)\n",
    "    - [Preprocessing data](#preprocessing-data)\n",
    "    - [Scaling the data by normalization](#scaling-the-data-by-normalization)\n",
    "    - [Ridge Regression: (With hypertuning to find α)](#ridge-regression-with-hypertuning-to-find-α)\n",
    "    - [Random Forest (Hypertuning parameters)](#random-forest-hypertuning-parameters)\n",
    "    - [XGB Regressor + Hypertuning parameters](#xgb-regressor--hypertuning-parameters)\n",
    "    - [Gradient Boosting Regressor + Hypertuning parameters](#gradient-boosting-regressor--hypertuning-parameters)\n",
    "    - [K-Fold Cross-Validation for all models](#k-fold-cross-validation-for-all-models)\n",
    "  - [ML: RQ: How significant is the relationship between latitude and temperature and can latitude be used to accurately predict temperatures?](#ml-rq-how-significant-is-the-relationship-between-latitude-and-temperature-and-can-latitude-be-used-to-accurately-predict-temperatures)\n",
    "  - [Visualizing temperature trends](#visualizing-temperature-trends)\n",
    "  - [Correlation matrix](#correlation-matrix)\n",
    "  - [Regression models - latitude only](#regression-models-latitude-only)\n",
    "  - [Regression models - laitutde + longitude](#regression-models-latitude-longitude)\n",
    "  - [Scatter plot and Hexbin - Actual vs Predicted](#Scatter-plot-Hexbin-Actuals-Predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d8d6b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ba344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = pd.read_csv(\"data/GlobalLandTemperaturesByCity.csv\")\n",
    "df_city['Date'] = pd.to_datetime(df_city['dt'], errors='coerce')\n",
    "df_city['Year'] = df_city['Date'].dt.year\n",
    "df_city = df_city.dropna(subset=['Date', 'AverageTemperature', 'Latitude', 'Longitude'])\n",
    "\n",
    "lat_values = df_city['Latitude'].str.extract(r'(\\d+\\.?\\d*)')[0].astype(float)\n",
    "is_south = df_city['Latitude'].str.endswith('S')\n",
    "df_city['Latitude'] = lat_values * np.where(is_south, -1, 1)\n",
    "\n",
    "long_values = df_city['Longitude'].str.extract(r'(\\d+\\.?\\d*)')[0].astype(float)\n",
    "is_west = df_city['Longitude'].str.endswith('W')\n",
    "df_city['Longitude'] = long_values * np.where(is_west, -1, 1)\n",
    "\n",
    "df_city = df_city[df_city['Year'] >= 1875]\n",
    "df_city['Latitude_bin'] = pd.cut(df_city['Latitude'], bins=np.arange(-90, 100, 10))\n",
    "\n",
    "print(df_city.head(3))\n",
    "print(df_city.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849bd943",
   "metadata": {},
   "source": [
    "### Data wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af28873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e97e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4efa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d917258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a842dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_country.describe(include = \"all\"))\n",
    "print(df.describe(include = 'all'))\n",
    "print(energy.describe(include = 'all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89ef6a",
   "metadata": {},
   "source": [
    "#### Quality checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_country.shape)\n",
    "print(\"Total countries:\", df_country['Country'].nunique())\n",
    "print(\"Total months:\", df_country['dt'].nunique())\n",
    "\n",
    "print(df.shape)\n",
    "print(energy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4cfeaa",
   "metadata": {},
   "source": [
    " `drop records`** reasonable to restrict the analysis to data from **1900 onward**(significantly reduces the sparsity and improves data reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328383f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country['dt'] = pd.to_datetime(df_country['dt'])\n",
    "df_country_clean = df_country[df_country['dt'].dt.year >=1900]\n",
    "df_country_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ebbce",
   "metadata": {},
   "source": [
    "#### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e891cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 1,\n",
    "                   thresh=int(len(df) * 0.3)  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65630e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "energy = energy.dropna(subset = ['iso_code'])\n",
    "energy['iso_code'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy[['renewables_consumption', 'primary_energy_consumption']].corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d997b7",
   "metadata": {},
   "source": [
    "### EDA: `df_country`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be624c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### a general function: choropleth map: better to see whether the data is well-cleaning or not.\n",
    "def plot_country_choropleth(df, country_col='Country', value_col=None, aggfunc='count',title='Choropleth Map by Country', color_scale='viridis'):\n",
    "    \"\"\"\n",
    "    Creates a choropleth map using Plotly based on country-level data.\n",
    "    \"\"\"\n",
    "    if value_col:\n",
    "        df_agg = df.groupby(country_col)[value_col].agg(aggfunc).reset_index()\n",
    "        df_agg.columns = [country_col, 'Value']\n",
    "    else:\n",
    "        df_agg = df[country_col].value_counts().reset_index()\n",
    "        df_agg.columns = [country_col, 'Value']\n",
    "\n",
    "    fig = px.choropleth(df_agg, locations=country_col, locationmode='country names', color='Value',\n",
    "        color_continuous_scale=color_scale, title=title\n",
    "    )\n",
    "    fig.update_layout(coloraxis_colorbar=dict(title='Value'),\n",
    "        margin=dict(l=0, r=0, t=50, b=0))\n",
    "    fig.show()\n",
    "\n",
    "    # plot_country_choropleth(df_country,\n",
    "                        # title='Number of Temperature Records by Country(All RECORDS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21086723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country['dt'] = pd.to_datetime(df_country['dt'])\n",
    "df_country_clean = df_country[df_country['dt'].dt.year >=1900]\n",
    "df_country_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f20554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8cd10",
   "metadata": {},
   "source": [
    "Because this is a time-series dataset, filling the < 1 % temperature gaps with an overall mean\n",
    "\n",
    "=> Safely remove the rows with missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_clean = df_country_clean.dropna(subset=['AverageTemperature'])\n",
    "# a choropleth map to show that the data are well-cleaning:\n",
    "# plot_country_choropleth(df_country_clean,\n",
    "#                         title='Number of Temperature Records by Country (from 1900)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_country_choropleth(df_country_clean,\n",
    "                        value_col= 'AverageTemperature',\n",
    "                        aggfunc = 'mean',\n",
    "                        title = 'Average Temperature of Countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808706a",
   "metadata": {},
   "source": [
    "#### RQ: What are the countries with minimum and maximum average temperature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_clean[df_country_clean['AverageTemperature']== df_country_clean['AverageTemperature'].min()]\n",
    "df_country_clean[df_country_clean['AverageTemperature']== df_country_clean['AverageTemperature'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde_temperature_comparison(df, countries, year):\n",
    "    \"\"\"\n",
    "    Plots KDE curves of average temperature for multiple countries in a given year.\n",
    "    Parameters:\n",
    "    - df: DataFrame with 'Country', 'dt', 'AverageTemperature'\n",
    "    - countries: list of country names, e.g. ['Kuwait', 'Greenland']\n",
    "    - year: int, e.g. 2012\n",
    "    \"\"\"\n",
    "    df = df[df['Country'].isin(countries)].copy()\n",
    "    df.dropna(subset=['AverageTemperature', 'dt'], inplace=True)\n",
    "    df['dt'] = pd.to_datetime(df['dt'])\n",
    "    df['year'] = df['dt'].dt.year\n",
    "    df = df[df['year'] == year]\n",
    "    custom_palette = {'Greenland': 'skyblue', 'Kuwait': 'orange'}\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(data=df,\n",
    "        x='AverageTemperature',\n",
    "        hue='Country',\n",
    "        palette=custom_palette,\n",
    "        fill=True,\n",
    "        common_norm=False,\n",
    "        alpha=0.5,\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.title(f'KDE of Average Temperature in {\", \".join(countries)} ({year})')\n",
    "    plt.xlabel('Temperature (°C)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plot_kde_temperature_comparison(df_country_clean, ['Greenland', 'Kuwait'], 2012)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5a884",
   "metadata": {},
   "source": [
    "#### RQ: How has the average yearly temperature changed from 1900 to 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d73b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_clean['Year'] = df_country_clean['dt'].dt.year\n",
    "df_yearly_avg = df_country_clean.groupby('Year')['AverageTemperature'].mean().reset_index()\n",
    "\n",
    "sns.lmplot(data=df_yearly_avg, x='Year', y='AverageTemperature', palette= 'viridis')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.title('Yearly Average Temperature')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Temperature (Celsius)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec1af4",
   "metadata": {},
   "source": [
    "The regression line in the plot **highlights a significant upward trend**, suggesting that the average temperature has steadily increased over the last century, consistent with **global warming patterns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d52f2b",
   "metadata": {},
   "source": [
    "### EDA for `df`(emissions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15920885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of sparse\n",
    "df_emissions = df[df['year']>=1900]\n",
    "df_emissions.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1f51d",
   "metadata": {},
   "source": [
    "Handling the missing values: drop the columns contain >= 30% missing data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5319df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emissions = df_emissions.dropna(\n",
    "    axis=1,\n",
    "    thresh=int(len(df_emissions) * 0.3)  \n",
    ")\n",
    "print(df_emissions.shape)\n",
    "print(df_emissions.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5da65",
   "metadata": {},
   "source": [
    "#### RQ: The correlation of co2 with economics and energy factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_emissions[['gdp','co2']].dropna(), x='gdp', y='co2', alpha=0.6)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('GDP (log scale)')\n",
    "plt.ylabel('CO2 emissions (log scale)')\n",
    "plt.title('GDP vs. CO2 emissions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86abb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_emissions, x='energy_per_capita', y='co2_per_capita', alpha=0.6)\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.title('Energy vs CO2 per capita (log–log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emissions['gdp_per_capita'] = df_emissions['gdp'] / df_emissions['population']\n",
    "\n",
    "df_emissions['gdp_per_capita'] = df_emissions['gdp_per_capita'].round(3) \n",
    "\n",
    "\n",
    "df_emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_emissions, x='gdp_per_capita', y='co2_per_capita', alpha = 0.6)\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.title('GDP vs CO2 per capita (log–log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e6205",
   "metadata": {},
   "source": [
    "#### RQ: Which regions or economic blocs were the top $CO_2$ emitters in the most recent year, based on OWID aggregate groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ec07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_emitters(df,start, end, level,  metric=\"co2\", top_n=5):\n",
    "    \"\"\"\n",
    "    Return the top-N emitters for a given period.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        OWID / GCP emissions table. Must contain columns\n",
    "        'year', 'country', 'iso_code', and the metric column.\n",
    "    start_year, end_year : int\n",
    "        Inclusive year boundaries (e.g. 2013, 2023).\n",
    "    level : {\"aggregate\", \"country\"}\n",
    "        aggregate – OWID/GCP group rows (iso_code is missing OR starts 'OWID_').\n",
    "        country   – ISO-3166 alpha-3 rows only (len == 3, all A-Z).\n",
    "    metric : str, default \"co2\"\n",
    "        Column to sum.\n",
    "    top_n : int, default 5\n",
    "        How many rows to return.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df.loc[df[\"iso_code\"] == \"\", \"iso_code\"] = np.nan\n",
    "\n",
    "    recent = df[(df[\"year\"] >= start) & (df[\"year\"] <= end)]\n",
    "\n",
    "    \n",
    "    if level == \"country\":\n",
    "        mask = (recent[\"iso_code\"].notna()\n",
    "            & recent[\"iso_code\"].str.fullmatch(r\"[A-Z]{3}\")\n",
    "        )\n",
    "    elif level == \"aggregate\":\n",
    "        mask = pd.Series(True, index=recent.index)\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'aggregate' or 'country'\")\n",
    "\n",
    "    subset = recent[mask]\n",
    "\n",
    "    cumulative = (subset.groupby(\"country\")[metric].sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(top_n)\n",
    "        .reset_index()\n",
    "        .rename(columns={metric: f\"cumulative_{metric}_{start}_{end}\"}) )\n",
    "\n",
    "    return cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe645ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_top5 = top_emitters(df_emissions, 2013, 2023, level=\"aggregate\")\n",
    "print(agg_top5)\n",
    "fig = px.bar(agg_top5, x=\"country\", y=\"cumulative_co2_2013_2023\", color=\"country\",  \n",
    "    text=\"cumulative_co2_2013_2023\",  \n",
    "    title=\"Cumulative CO2 Emissions (2013–2023) by Aggregation\",\n",
    "    labels={\"cumulative_co2_2013_2023\": \"Cumulative CO2 (Mt)\", \"country\": \"Aggregation\"},\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text:.2f}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b5f0c",
   "metadata": {},
   "source": [
    "#### RQ: Which countries are the top emitters in the recent ten years? (2013-2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 individual countries, 2013-2023\n",
    "country_top5 = top_emitters(df_emissions, 2013, 2023, level=\"country\")\n",
    "print(country_top5)\n",
    "fig = px.bar(\n",
    "    country_top5,  \n",
    "    x=\"country\",\n",
    "    y=\"cumulative_co2_2013_2023\",\n",
    "    color=\"country\",  \n",
    "    text=\"cumulative_co2_2013_2023\",  \n",
    "    title=\"Cumulative CO₂ Emissions (2013–2023) by Country\",\n",
    "    labels={\"cumulative_co2_2013_2023\": \"Cumulative CO2 (Mt)\", \"country\": \"Country\"},\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text:.2f}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465685e9",
   "metadata": {},
   "source": [
    "#### Temperature change from $CO_2$ by continents from 1950 to 2020:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c784ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only continent-level data\n",
    "continents = [\"Africa\", \"Asia\", \"Europe\", \"North America\", \"South America\", \"Oceania\"]\n",
    "df_continents = df_emissions[\n",
    "    df_emissions[\"country\"].isin(continents) &\n",
    "    df_emissions[\"temperature_change_from_co2\"].notna()\n",
    "]\n",
    "\n",
    "# Optional: recent years only\n",
    "df_continents = df_continents[df_continents[\"year\"] >= 1950]\n",
    "\n",
    "# Plot\n",
    "fig = px.line(df_continents, x=\"year\", y=\"temperature_change_from_co2\", color=\"country\",\n",
    "              title=\"Temperature Change from CO₂ by Continent (1950–present)\",\n",
    "              labels={\"country\": \"Continent\", \"temperature_change_from_co2\": \"Temp Change (°C)\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7587db",
   "metadata": {},
   "source": [
    "#### Average Temperature and $CO_2$ emissions through years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure 'dt' is datetime\n",
    "df_country_clean[\"dt\"] = pd.to_datetime(df_country_clean[\"dt\"])\n",
    "\n",
    "# 2. Extract year\n",
    "df_country_clean[\"year\"] = df_country_clean[\"dt\"].dt.year\n",
    "\n",
    "# 3. Group by year and average both temperature and uncertainty\n",
    "df_yearly = df_country_clean.groupby(\"year\")[[\"AverageTemperature\", \"AverageTemperatureUncertainty\"]].mean().reset_index()\n",
    "\n",
    "# 4. Plot with confidence band\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(\n",
    "    x=\"year\",\n",
    "    y=\"AverageTemperature\",\n",
    "    data=df_yearly,\n",
    "    label=\"Avg Temperature\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    df_yearly[\"year\"],\n",
    "    df_yearly[\"AverageTemperature\"] - df_yearly[\"AverageTemperatureUncertainty\"],\n",
    "    df_yearly[\"AverageTemperature\"] + df_yearly[\"AverageTemperatureUncertainty\"],\n",
    "    alpha=0.3,\n",
    "    label=\"+- Uncertainty\"\n",
    ")\n",
    "\n",
    "plt.title(\"Average Temperature through Years\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Temperature (°C)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CO2 emissions trends over the years\n",
    "# Keep only the country no more aggregation\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.lineplot(x=\"year\", y=\"co2\", data=df_emissions)\n",
    "plt.title('CO2 emission trends over the years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a07b60",
   "metadata": {},
   "source": [
    "#### $CO_2$ emissions over time of top 5 emitters countries since 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d64031",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_countries = [\"China\",\"Russia\", \"United States\", \"Japan\", \"India\"]\n",
    "df_filtered = df_emissions[\n",
    "    (df_emissions[\"country\"].isin(selected_countries)) &\n",
    "    (df_emissions[\"co2\"].notna())\n",
    "]\n",
    "fig = px.box(\n",
    "    df_filtered[df_filtered[\"year\"] >= 1990],\n",
    "    x=\"country\",\n",
    "    y=\"co2\",\n",
    "    log_y=True,                 # compresses huge differences\n",
    "    points='outliers',          # show only true outliers\n",
    "    title=\"CO₂ Emissions Since 1990 (log scale)\",\n",
    "    labels={\"co2\": \"CO₂ Emissions (million tonnes, log scale)\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231c98c",
   "metadata": {},
   "source": [
    "#### CO2 emission per capita trends over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ab4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "sns.lineplot(x=\"year\", y=\"co2_per_capita\", data=df)\n",
    "plt.title('CO2 emission per capita trends')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342e772",
   "metadata": {},
   "source": [
    "## SDA: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c02b3a",
   "metadata": {},
   "source": [
    "### Has the average annual temperature significantly increased when comparing the early 20th century to the early 21st century\n",
    "- Testing hypothesis: \n",
    "\n",
    "    $H_0$: $\\mu_{\\text{early}} = \\mu_{\\text{recent}}$\n",
    "\n",
    "    $H_a$: $\\mu_{\\text{early}} \\ne \\mu_{\\text{recent}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0901a",
   "metadata": {},
   "source": [
    "#### Diagnostics plots and tests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca388f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals vs Fitted plot\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import linregress\n",
    "\n",
    "result = linregress(df_yearly_avg['Year'], df_yearly_avg['AverageTemperature'])\n",
    "print(result)\n",
    "# Fit model using statsmodels\n",
    "X = df_yearly_avg['Year']\n",
    "y = df_yearly_avg['AverageTemperature']\n",
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "# 1. Residuals vs Fitted\n",
    "residuals = model.resid\n",
    "fitted = model.fittedvalues\n",
    "sns.residplot(x=fitted, y=residuals, lowess=True)\n",
    "plt.axhline(0, linestyle='--')\n",
    "plt.title('Residuals vs. Fitted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durbin-Watson test\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_stat = durbin_watson(model.resid)\n",
    "print(f\"Durbin-Watson statistic: {dw_stat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d14a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normal Q-Q Plot\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title('Normal Q-Q')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e229c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns and drop NaNs\n",
    "df_temp = df_yearly_avg[['Year', 'AverageTemperature']].dropna()\n",
    "\n",
    "# Define periods\n",
    "early = df_temp[(df_temp['Year'] >= 1900) & (df_temp['Year'] <= 1920)]['AverageTemperature']\n",
    "recent = df_temp[(df_temp['Year'] >= 2000) & (df_temp['Year'] <= 2020)]['AverageTemperature']\n",
    "\n",
    "# Create Q-Q plots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sm.qqplot(early, line='s', ax=axes[0])\n",
    "axes[0].set_title('Q-Q Plot: 1900–1920')\n",
    "\n",
    "sm.qqplot(recent, line='s', ax=axes[1])\n",
    "axes[1].set_title('Q-Q Plot: 2000–2020')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7ec77",
   "metadata": {},
   "source": [
    "#### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "# 2-sample t-tests\n",
    "early = df_yearly_avg[df_yearly_avg['Year'] < 1950]['AverageTemperature']\n",
    "recent = df_yearly_avg[df_yearly_avg['Year'] >= 1950]['AverageTemperature']\n",
    "\n",
    "t_stat, p_value = ttest_ind(early, recent, equal_var=False)  \n",
    "print(f\"t = {t_stat:.3f}, p-value = {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1653d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "early = df_yearly_avg.query(\"1900 <= Year <= 1920\").copy()\n",
    "recent = df_yearly_avg.query(\"2000 <= Year <= 2020\").copy()\n",
    "\n",
    "early[\"Period\"] = \"1900–1920\"\n",
    "recent[\"Period\"] = \"2000–2020\"\n",
    "plot_df = pd.concat([early, recent], ignore_index=True)\n",
    "sns.kdeplot(data=plot_df, x=\"AverageTemperature\", hue=\"Period\", fill=True, common_norm=False)\n",
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "early   = df_yearly_avg.query(\"Year.between(1900, 1920)\")[\"AverageTemperature\"]\n",
    "recent  = df_yearly_avg.query(\"Year.between(2000, 2020)\")[\"AverageTemperature\"]\n",
    "# Normality checks\n",
    "for label, series in {\"Early\": early, \"Recent\": recent}.items():\n",
    "    stat, p = scipy.stats.shapiro(series)\n",
    "    print(f\"{label} Shapiro p = {p:.3f}\")\n",
    "\n",
    "# Variance equality\n",
    "levene_p = scipy.stats.levene(early, recent).pvalue\n",
    "print(f\"Levene p = {levene_p:.3f}\")\n",
    "\n",
    "# Welch two-sample t-test\n",
    "t_stat, p_val = scipy.stats.ttest_ind(early, recent, equal_var=False)\n",
    "print(f\"Welch t = {t_stat:.2f}, p = {p_val:.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e7081",
   "metadata": {},
   "source": [
    "### SDA: Monika"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521444",
   "metadata": {},
   "source": [
    "## Machine Learning models: For 3 main research questions: \n",
    "- (Duncan)\n",
    "- (Oscar)\n",
    "- How accurately can a machine-learning model predict a country’s total CO₂ emissions from its share of renewable-energy consumption (and other socio-economic indicators)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471200a",
   "metadata": {},
   "source": [
    "###  ML: RQ: How accurately can a machine-learning model predict a country’s total CO₂ emissions from its share of renewable-energy consumption (and other socio-economic indicators)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a8c5d",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the country no more aggregation\n",
    "df = df.dropna(subset=['iso_code'])\n",
    "# indicators = ['gdp', 'population', 'primary_energy_consumption','renewables_consumption', 'oil_consumption' ]\n",
    "keys = ['country', 'year']\n",
    "df_sub     = df[ keys + ['gdp', 'population', 'co2','primary_energy_consumption' ]]\n",
    "energy_sub = energy[ keys + ['renewables_consumption','oil_consumption'] ]\n",
    "\n",
    "merged = pd.merge(df_sub, energy_sub, on=keys, how='inner')\n",
    "# Cook's distance\n",
    "influential_countries = ['China', 'India', 'United States']\n",
    "\n",
    "# Filter out these countries\n",
    "merged = merged[~merged['country'].isin(influential_countries)]\n",
    "\n",
    "merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41523d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop sparse countries\n",
    "core = ['gdp','population','primary_energy_consumption','co2']\n",
    "coverage = (\n",
    "    merged.assign(nonmiss=lambda d: d[core].notnull().all(axis=1))\n",
    "      .groupby('country')['nonmiss'].mean()\n",
    ")\n",
    "keep = coverage[coverage >= 0.6].index\n",
    "merged = merged[merged['country'].isin(keep)].copy()\n",
    "# Imputation for other missing records per country\n",
    "cols_to_impute = ['gdp', 'primary_energy_consumption','oil_consumption', 'renewables_consumption']\n",
    "# 3. Per-country median imputation\n",
    "merged[cols_to_impute] = merged.groupby('country')[cols_to_impute] \\\n",
    "                       .transform(lambda x: x.fillna(x.median()))\n",
    "# fill 0 for oil_consumption, and renewable\n",
    "merged['renewables_consumption'] = merged['renewables_consumption'].fillna(0)\n",
    "merged['oil_consumption'] = merged['oil_consumption'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "merged['year'] = merged['year'].astype(int)\n",
    "prediction_features = ['year', 'gdp','population','primary_energy_consumption','renewables_consumption','oil_consumption']\n",
    "target = 'co2'\n",
    "X = merged[prediction_features]\n",
    "y = merged[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, shuffle=False\n",
    ")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(\"\\nFeature columns:\", X_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7c1be",
   "metadata": {},
   "source": [
    "#### Scaling the data by normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e503492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0686c0f",
   "metadata": {},
   "source": [
    "#### Ridge Regression: (With hypertuning to find $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finding alpha \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "alphas = np.linspace(0.01, 10, 50) \n",
    "ridge_cv = RidgeCV(alphas=alphas, scoring='neg_root_mean_squared_error', cv=5)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best alpha:\", ridge_cv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(coefs.shape[1]):\n",
    "    plt.plot(alphas, coefs[:, i], label=X_train.columns[i])\n",
    "\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Ridge Coefficients vs. Regularization Strength (lambda)\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fd6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ridge:\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "#Model\n",
    "ridge = Ridge(alpha = 0.01 )\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "# Metrics performance\n",
    "rmse = root_mean_squared_error(y_test, y_pred_ridge)\n",
    "r2 = r2_score(y_test, y_pred_ridge)\n",
    "mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1) Extract absolute coefficients and feature names\n",
    "importance = np.abs(ridge.coef_)\n",
    "feature_names = X_train.columns\n",
    "\n",
    "df_imp = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=True)  # ascending for horizontal bars\n",
    "\n",
    "# 3) Plot\n",
    "plt.barh(df_imp['feature'], df_imp['importance'])\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Feature Importances (Ridge Regression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Actual vs Predicted\n",
    "df1 = pd.DataFrame({\n",
    "    'Actual CO2': y_test,\n",
    "    'Predicted CO2': y_pred_ridge\n",
    "})\n",
    "fig1 = px.scatter(\n",
    "    df1, x='Actual CO2', y='Predicted CO2',\n",
    "    trendline='ols',\n",
    "    trendline_color_override='black',\n",
    "    labels={\n",
    "      'Actual CO2':'Actual CO2 Emissions',\n",
    "      'Predicted CO2':'Predicted CO2 Emissions'\n",
    "    },\n",
    "    title='Actual vs. Predicted CO2 Emissions'\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "\n",
    "# 2) Residuals vs Fitted\n",
    "residuals = y_test - y_pred_ridge\n",
    "df2 = pd.DataFrame({\n",
    "    'Fitted CO2': y_pred_ridge,\n",
    "    'Residuals': residuals\n",
    "})\n",
    "fig2 = px.scatter(\n",
    "    df2, x='Fitted CO2', y='Residuals',\n",
    "    labels={\n",
    "      'Fitted CO2':'Predicted (Fitted) CO2 of Ridge Regressions',\n",
    "      'Residuals':'Residual'\n",
    "    },\n",
    "    title='Residuals vs. Fitted Values'\n",
    ")\n",
    "# add a horizontal zero‐error line\n",
    "fig2.add_hline(y=0, line_dash='dash', line_color='black')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62046c",
   "metadata": {},
   "source": [
    "#### Random Forest( Hypertuning parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [ None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'max_features': ['auto', 'sqrt', 0.5]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_dist,\n",
    "    n_iter=30,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best params:\", search.best_params_)\n",
    "best_rf = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    n_jobs=-1,\n",
    "    min_samples_split= 2,\n",
    "    min_samples_leaf= 1,\n",
    "    max_features='sqrt'\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "# Metrics performance\n",
    "rmse = root_mean_squared_error(y_test, y_pred_rf)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "\n",
    "# 2) Compute importances and their std across all trees\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# 3) Sort features by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X_train.columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) Plot with error bars\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Feature importances (with std)\")\n",
    "plt.bar(\n",
    "    range(len(importances)),\n",
    "    importances[indices],\n",
    "    color=\"skyblue\",\n",
    "    yerr=std[indices],\n",
    "    align=\"center\"\n",
    ")\n",
    "plt.xticks(range(len(importances)), feature_names[indices], rotation=45, ha='right')\n",
    "plt.ylabel(\"Mean importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a253d4",
   "metadata": {},
   "source": [
    "#### XGB Regressor + Hypertuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bff333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# XGBoost model\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,       # Lower learning rate for better generalization\n",
    "    max_depth=5,\n",
    "    reg_alpha=0.5,      # L1 regularization     \n",
    "    reg_lambda=1.0,      # L2 regularzation    \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "# Output\n",
    "print(f\"XGB RMSE: {rmse:.3f}\")\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b3084",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor + Hypertuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize Gradient Boosting Regressor with similar parameters\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "gbr.fit(X_train_scaled, y_train)\n",
    "y_pred_gbr = gbr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
    "mae_gbr = mean_absolute_error(y_test, y_pred_gbr)\n",
    "\n",
    "# Compute feature importances\n",
    "importances_gbr = gbr.feature_importances_\n",
    "indices_gbr = np.argsort(importances_gbr)[::-1]\n",
    "feature_names_gbr = X_train.columns\n",
    "print(f\" RMSE: {rmse_gbr:.3f}\")\n",
    "print(f\"R²: {r2_gbr:.3f}\")\n",
    "print(f\"MAE: {mae_gbr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc145f3",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "models = {'ridge': ridge, 'rf': rf, 'xgb': xgb, 'gbr': gbr}\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    # 5-fold CV with R²\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, scoring='r2', cv=5)\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name} R_squared scores: {scores}\")\n",
    "    print(f\"{name} mean R_squared:   {scores.mean():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb553431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learning_curve_px(model, name):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model,\n",
    "        X_train_scaled, y_train,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        n_jobs=-1\n",
    "        )\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    val_mean   = val_scores.mean(axis=1)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'training_set_size': np.concatenate([train_sizes, train_sizes]),\n",
    "        'r2_score':          np.concatenate([train_mean, val_mean]),\n",
    "        'split':             ['train'] * len(train_sizes) + ['validation'] * len(train_sizes)\n",
    "    })\n",
    "\n",
    "    fig = px.line(\n",
    "        df, x='training_set_size',y='r2_score',color='split',\n",
    "        markers=True,\n",
    "        title=f'Learning Curve: {name}'\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Training Set Size',\n",
    "        yaxis_title='R_squared Score',\n",
    "        legend_title=''\n",
    "    )\n",
    "    fig.show()\n",
    "plot_learning_curve_px(ridge, 'Ridge Regression')\n",
    "plot_learning_curve_px(rf,    'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88ba95",
   "metadata": {},
   "source": [
    "# ML: RQ: How significant is the relationship between latitude and temperature and can latitude be used to accurately predict temperatures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b66dbe",
   "metadata": {},
   "source": [
    "## Visualizing Temperature Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city['Year_bin'] = (df_city['Year'] // 10) * 10  \n",
    "grouped = df_city.groupby(['Year_bin', 'Latitude_bin'], observed=True)['AverageTemperature'].mean().reset_index()\n",
    "baseline = grouped.groupby('Latitude_bin', observed=True)['AverageTemperature'].mean().reset_index()\n",
    "baseline.columns = ['Latitude_bin', 'BaselineTemp']\n",
    "grouped = grouped.merge(baseline, on='Latitude_bin', how='left')\n",
    "grouped['TempAnomaly'] = grouped['AverageTemperature'] - grouped['BaselineTemp']\n",
    "anomaly_data = grouped.pivot(index='Latitude_bin', columns='Year_bin', values='TempAnomaly')\n",
    "anomaly_data = anomaly_data.sort_index(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(anomaly_data, cmap=\"coolwarm\", center=0, \n",
    "            cbar_kws={'label': 'Temperature Difference Compared to Average (°C)'}, \n",
    "            linewidths=0.2)\n",
    "plt.title(\"Temperature Difference Heatmap by Latitude and Year\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c43f0c",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2620b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_1950 = df_city[df_city['Year'] >= 1950].copy()\n",
    "data = df_after_1950[['AverageTemperature', 'Latitude', 'Longitude']].copy()\n",
    "correlation_matrix = data.corr(method='pearson')  \n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation with Temperature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03af7b",
   "metadata": {},
   "source": [
    "## Regression Models - Latitude Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_latitude = df_after_1950[['Latitude']]\n",
    "features_latitude_longitude = df_after_1950[['Latitude', 'Longitude']]\n",
    "temperature = df_after_1950['AverageTemperature']\n",
    "\n",
    "X_train_latitude, X_test_latitude, y_train, y_test = train_test_split(features_latitude, temperature, test_size=0.2, random_state=42)\n",
    "X_train_both_features, X_test_both_features, _, _ = train_test_split(features_latitude_longitude, temperature, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression Model\": LinearRegression(),\n",
    "    \"Random Forest Model\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"Gradient Boosting Model\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost Model\": XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "print(\"Model Performance (Latitude Only):\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_latitude, y_train)\n",
    "    preds = model.predict(X_test_latitude)\n",
    "    rmse = root_mean_squared_error(y_test, preds)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    results.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
    "    print(f\"{name}: RMSE = {rmse:.2f} °C | MAE = {mae:.2f} °C | R² = {r2:.2f}\")\n",
    "\n",
    "best_model_result = min(results, key=lambda x: x['RMSE'])\n",
    "best_model_name = best_model_result['Model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest Model (Latitude): {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662df0d",
   "metadata": {},
   "source": [
    "## Regression Models - Latitude + Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train_both_features, y_train)\n",
    "predictions_latitude_longitude = best_model.predict(X_test_both_features)\n",
    "rmse_latitude_longitude = root_mean_squared_error(y_test, predictions_latitude_longitude)\n",
    "r2_latitude_longitude = r2_score(y_test, predictions_latitude_longitude)\n",
    "mae_latitude_longitude = mean_absolute_error(y_test, predictions_latitude_longitude)\n",
    "\n",
    "print(f\"\\n{best_model_name} Performance (Latitude + Longitude):\")\n",
    "print(f\"RMSE = {rmse_latitude_longitude:.2f} °C | R² = {r2_latitude_longitude:.2f} | MAE = {mae_latitude_longitude:.2f} °C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7c57f",
   "metadata": {},
   "source": [
    "## Scatter Plot and Hexbin - Actuals vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hexbin\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hexbin(y_test, predictions_latitude_longitude, gridsize=50, cmap='viridis', bins='log')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.xlabel(\"Actual Temperatures (°C)\")\n",
    "plt.ylabel(\"Predicted Temperatures (°C)\")\n",
    "plt.title(f\"{best_model_name} Predictions vs Actuals\")\n",
    "plt.colorbar(label='log(N)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot\n",
    "df = pd.DataFrame({\n",
    "    'Actual Temperatures (°C)': y_test,\n",
    "    'Predicted Temperatures (°C)': predictions_latitude_longitude\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='Actual Temperatures (°C)',\n",
    "    y='Predicted Temperatures (°C)',\n",
    "    trendline_color_override='red',\n",
    "    labels={\n",
    "        'Actual Temperatures (°C)': 'Actual Temperatures (°C)',\n",
    "        'Predicted Temperatures (°C)': 'Predicted Temperatures (°C)'\n",
    "    },\n",
    "    title=f'{best_model_name} Predictions vs Actuals'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0508c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/owid-co2-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gdp_per_capita'] = df['gdp'] / df['population']\n",
    "\n",
    "columns_of_interest = [\n",
    "    'co2', 'co2_per_capita', 'gdp', 'gdp_per_capita', \n",
    "    'energy_per_capita', 'coal_co2', 'oil_co2', 'gas_co2',\n",
    "    'cement_co2', 'land_use_change_co2', 'consumption_co2'\n",
    "]\n",
    "\n",
    "continental_entities = [\n",
    "    'Africa', \n",
    "    'Asia', \n",
    "    'Europe', \n",
    "    'European Union (28)',\n",
    "    'North America',\n",
    "    'South America',\n",
    "    'Oceania'\n",
    "]\n",
    "df_copy = df.copy(deep=True)\n",
    "df_continental = df_copy[df_copy['country'].isin(continental_entities)]\n",
    "# remove the gdp and gdp_per_capita columns from the df_continental\n",
    "df_continental = df_continental[columns_of_interest].drop(columns=['gdp', 'gdp_per_capita'])\n",
    "\n",
    "df = df.dropna(subset=columns_of_interest)\n",
    "\n",
    "\n",
    "\n",
    "income_group_entities = [\n",
    "    'High-income countries', 'Low-income countries',\n",
    "    'Lower-middle-income countries', 'Upper-middle-income countries',\n",
    "    'Least developed countries (Jones et al.)'\n",
    "]\n",
    "\n",
    "df_income_group = df[df['country'].isin(income_group_entities)].copy()\n",
    "\n",
    "countries = [\n",
    "    'Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola',\n",
    "    'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria',\n",
    "    'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados',\n",
    "    'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan',\n",
    "    'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei',\n",
    "    'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon',\n",
    "    'Canada', 'Cape Verde', 'Central African Republic', 'Chad', 'Chile',\n",
    "    'China', 'Colombia', 'Comoros', 'Congo', 'Costa Rica',\n",
    "    \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Cyprus', 'Czechia',\n",
    "    'Democratic Republic of Congo', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic',\n",
    "    'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea',\n",
    "    'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland',\n",
    "    'France', 'Gabon', 'Gambia', 'Georgia', 'Germany',\n",
    "    'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea',\n",
    "    'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hungary',\n",
    "    'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq',\n",
    "    'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan',\n",
    "    'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait',\n",
    "    'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho',\n",
    "    'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg',\n",
    "    'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali',\n",
    "    'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico',\n",
    "    'Micronesia (country)', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro',\n",
    "    'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru',\n",
    "    'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger',\n",
    "    'Nigeria', 'North Korea', 'North Macedonia', 'Norway', 'Oman',\n",
    "    'Pakistan', 'Palau', 'Panama', 'Papua New Guinea', 'Paraguay',\n",
    "    'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar',\n",
    "    'Romania', 'Russia', 'Rwanda', 'Saint Kitts and Nevis', 'Saint Lucia',\n",
    "    'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia',\n",
    "    'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore',\n",
    "    'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa',\n",
    "    'South Korea', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan',\n",
    "    'Suriname', 'Sweden', 'Switzerland', 'Syria', 'Tajikistan',\n",
    "    'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Tonga',\n",
    "    'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Tuvalu',\n",
    "    'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States',\n",
    "    'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam',\n",
    "    'Yemen', 'Zambia', 'Zimbabwe'\n",
    "]\n",
    "\n",
    "df_countries = df[df['country'].isin(countries)].copy()\n",
    "\n",
    "def safe_qcut(group):\n",
    "    try:\n",
    "        return pd.qcut(group, q=3, labels=['low', 'middle', 'high'], duplicates='drop')\n",
    "    except ValueError:\n",
    "        # For countries with insufficient unique values, assign a default label\n",
    "        return pd.Series(['low'] * len(group), index=group.index)\n",
    "\n",
    "df_countries['income_bin'] = df_countries.groupby('country')['gdp_per_capita'].transform(safe_qcut)\n",
    "\n",
    "\n",
    "df_world = df_countries.groupby(['year']).agg({\n",
    "    'co2': 'sum',\n",
    "    'co2_per_capita': 'mean',\n",
    "    'gdp': 'sum',\n",
    "    'gdp_per_capita': 'mean',\n",
    "    'energy_per_capita': 'mean',\n",
    "    'coal_co2': 'sum',\n",
    "    'oil_co2': 'sum',\n",
    "    'gas_co2': 'sum',\n",
    "    'cement_co2': 'sum',\n",
    "    'land_use_change_co2': 'sum',\n",
    "    'consumption_co2': 'sum'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yearly_terciles(df, income_col='gdp_per_capita', \n",
    "                            year_col='year', country_col='country'):\n",
    "    df = df.copy()\n",
    "    df['income_bin'] = np.nan\n",
    "    df = df[(df[income_col].notna()) & (df[income_col] > 0)]\n",
    "\n",
    "    for year in df[year_col].unique():\n",
    "        year_mask = df[year_col] == year\n",
    "        year_data = df[year_mask].copy()\n",
    "        \n",
    "        # Sort by GDP values (ascending)\n",
    "        year_data = year_data.sort_values(income_col)\n",
    "        \n",
    "        # Calculate GDP thresholds that divide into equal groups\n",
    "        n = len(year_data)\n",
    "        low_threshold = year_data[income_col].iloc[n//3 - 1]  # -1 because Python is 0-indexed\n",
    "        high_threshold = year_data[income_col].iloc[2*n//3 - 1]\n",
    "        \n",
    "        # Assign bins based on GDP values\n",
    "        df.loc[year_mask, 'income_bin'] = np.where(\n",
    "            df.loc[year_mask, income_col] <= low_threshold, 'low',\n",
    "            np.where(\n",
    "                df.loc[year_mask, income_col] <= high_threshold, 'middle', 'high'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Verification for debugging\n",
    "        if year == df[year_col].min():\n",
    "            print(f\"\\n=== Year {year} Bin Assignment ===\")\n",
    "            print(f\"Low threshold GDP: {low_threshold:.2f}\")\n",
    "            print(f\"High threshold GDP: {high_threshold:.2f}\")\n",
    "            print(\"\\nSample assignments:\")\n",
    "            sample = df[year_mask].sort_values(income_col)\n",
    "            print(sample[[country_col, income_col, 'income_bin']].head(3))\n",
    "            print(sample[[country_col, income_col, 'income_bin']].tail(3))\n",
    "    \n",
    "    # Ensure correct categorical ordering\n",
    "    df['income_bin'] = pd.Categorical(\n",
    "        df['income_bin'], \n",
    "        categories=['low', 'middle', 'high'], \n",
    "        ordered=True\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_income_mobility(df, country_col='country', year_col='year'):\n",
    "\n",
    "    # Ensure proper sorting\n",
    "    df = df.sort_values([country_col, year_col])\n",
    "    \n",
    "    # 1. Calculate transition statistics\n",
    "    transitions = df.groupby(country_col)['income_bin'].agg([\n",
    "        ('initial_bin', 'first'),\n",
    "        ('final_bin', 'last'),\n",
    "        ('unique_bins', 'nunique'),\n",
    "        ('max_bin', 'max'),\n",
    "        ('min_bin', 'min')\n",
    "    ])\n",
    "    \n",
    "    # Calculate mobility direction\n",
    "    transitions['mobility'] = np.where(\n",
    "        transitions['initial_bin'] < transitions['final_bin'], 'up',\n",
    "        np.where(transitions['initial_bin'] > transitions['final_bin'], 'down', 'stable'))\n",
    "    \n",
    "    # 2. Verify tercile distribution\n",
    "    print(\"=== Yearly Bin Distribution Verification ===\")\n",
    "    yearly_counts = pd.crosstab(df[year_col], df['income_bin'])\n",
    "    print(yearly_counts)\n",
    "    \n",
    "    # Check if counts are balanced\n",
    "    if not all(yearly_counts.nunique(axis=1) == 1):\n",
    "        print(\"\\nWarning: Some years have unequal bin counts due to total country count not being divisible by 3\")\n",
    "    \n",
    "    # 3. Visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot 1: Distribution of income bins over time\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.countplot(data=df, x=year_col, hue='income_bin', palette='viridis')\n",
    "    plt.title('Income Bin Distribution by Year')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot 2: Mobility summary\n",
    "    plt.subplot(2, 2, 2)\n",
    "    transitions['mobility'].value_counts().plot(kind='bar', color=['green', 'gray', 'red'])\n",
    "    plt.title('Overall Mobility Direction')\n",
    "    plt.xlabel('Mobility Type')\n",
    "    \n",
    "    # Plot 3: Example trajectories\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sample_countries = ['Poland', 'United States', 'China', 'India', 'Brazil']\n",
    "    for country in sample_countries:\n",
    "        country_data = df[df[country_col] == country]\n",
    "        plt.plot(country_data[year_col].astype(str), \n",
    "                country_data['income_bin'].cat.codes, \n",
    "                marker='o', label=country)\n",
    "    \n",
    "    plt.yticks(ticks=[0, 1, 2], labels=['low', 'middle', 'high'])\n",
    "    plt.title('Country Income Trajectories')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Income Bin')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "# Calculate yearly income bins\n",
    "df_with_bins = calculate_yearly_terciles(df_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f301e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmaps(df, group_col=None):\n",
    "    # Define feature sets\n",
    "    general_features = ['co2', 'gdp', 'coal_co2', 'oil_co2', \n",
    "                      'gas_co2', 'cement_co2', 'energy']\n",
    "    \n",
    "    per_capita_features = ['co2_per_capita', 'gdp_per_capita', \n",
    "                         'energy_per_capita', 'coal_co2_per_capita',\n",
    "                         'oil_co2_per_capita', 'gas_co2_per_capita',\n",
    "                         'cement_co2_per_capita']\n",
    "    \n",
    "    # Filter to only include features that exist in the dataframe\n",
    "    general_features = [f for f in general_features if f in df.columns]\n",
    "    per_capita_features = [f for f in per_capita_features if f in df.columns]\n",
    "    \n",
    "    if not general_features and not per_capita_features:\n",
    "        print(\"No valid features found for correlation analysis\")\n",
    "        return\n",
    "    \n",
    "    if group_col is None:\n",
    "        # Create figure for global correlations\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot general correlations\n",
    "        if general_features:\n",
    "            corr_general = df[general_features].corr()\n",
    "            sns.heatmap(corr_general, annot=True, cmap='coolwarm', center=0,\n",
    "                      fmt='.2f', annot_kws={'size': 9}, ax=ax1)\n",
    "            ax1.set_title('General Metrics Correlations', fontsize=20)\n",
    "            ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Plot per capita correlations\n",
    "        if per_capita_features:\n",
    "            corr_per_capita = df[per_capita_features].corr()\n",
    "            sns.heatmap(corr_per_capita, annot=True, cmap='coolwarm', center=0,\n",
    "                       fmt='.2f', annot_kws={'size': 9}, ax=ax2)\n",
    "            ax2.set_title('Per Capita Metrics Correlations', fontsize=20)\n",
    "            ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        plt.suptitle('Global Correlation Matrices', fontsize=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Grouped correlations\n",
    "        if group_col not in df.columns:\n",
    "            print(f\"Grouping column '{group_col}' not found in DataFrame\")\n",
    "            return\n",
    "            \n",
    "        for name, group in df.groupby(group_col):\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "            \n",
    "            # Plot general correlations\n",
    "            if general_features:\n",
    "                corr_general = group[general_features].corr()\n",
    "                sns.heatmap(corr_general, annot=True, cmap='coolwarm', center=0,\n",
    "                           fmt='.2f', annot_kws={'size': 9}, ax=ax1)\n",
    "                ax1.set_title(f'General Metrics for {name}', fontsize=20)\n",
    "                ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Plot per capita correlations\n",
    "            if per_capita_features:\n",
    "                corr_per_capita = group[per_capita_features].corr()\n",
    "                sns.heatmap(corr_per_capita, annot=True, cmap='coolwarm', center=0,\n",
    "                            fmt='.2f', annot_kws={'size': 14}, ax=ax2)\n",
    "                ax2.set_title(f'Per Capita Metrics for {name}', fontsize=20)\n",
    "                ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "            plt.suptitle(f'Correlation Matrices for {group_col}: {name}', fontsize=20)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def plot_per_capita_correlations_by_income(df, income_col='income_bin'):\n",
    "    # Define our specific per capita metrics of interest\n",
    "    per_capita_metrics = [\n",
    "        'co2_per_capita',\n",
    "        'gdp_per_capita', \n",
    "        'energy_per_capita'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include metrics that exist in the dataframe\n",
    "    per_capita_metrics = [m for m in per_capita_metrics if m in df.columns]\n",
    "    \n",
    "    if not per_capita_metrics:\n",
    "        print(\"No per capita metrics found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    if income_col not in df.columns:\n",
    "        print(f\"Income group column '{income_col}' not found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # Get unique income groups\n",
    "    income_groups = df[income_col].unique()\n",
    "    \n",
    "    # Create a figure with subplots for each income group\n",
    "    fig, axes = plt.subplots(1, len(income_groups), figsize=(6*len(income_groups), 5))\n",
    "    \n",
    "    # If only one income group, axes won't be an array\n",
    "    if len(income_groups) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, income_group in zip(axes, income_groups):\n",
    "        # Filter data for this income group\n",
    "        group_data = df[df[income_col] == income_group]\n",
    "        \n",
    "        # Calculate correlations\n",
    "        corr_matrix = group_data[per_capita_metrics].corr()\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(\n",
    "            corr_matrix,\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt='.2f',\n",
    "            annot_kws={'size': 15},\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        ax.set_title(\n",
    "            f'{income_group}\\n(n={len(group_data)})',\n",
    "            fontsize=20,\n",
    "            pad=20\n",
    "        )\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        ax.set_xticklabels(\n",
    "            ax.get_xticklabels(),\n",
    "            rotation=45,\n",
    "            ha='right',\n",
    "            fontsize=12\n",
    "        )\n",
    "        \n",
    "        # Rotate y-axis labels\n",
    "        ax.set_yticklabels(\n",
    "            ax.get_yticklabels(),\n",
    "            rotation=0,\n",
    "            fontsize=12\n",
    "        )\n",
    "    \n",
    "    plt.suptitle(\n",
    "        'Per Capita Metrics Correlation by Income Group',\n",
    "        fontsize=22,\n",
    "        y=1.05\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc069db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_heatmaps(df_world)\n",
    "plot_per_capita_correlations_by_income(df_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c77d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def run_regression_models(df, group_col, features=None, target=None):\n",
    "    if features is None:\n",
    "        features = ['gdp_per_capita', 'energy_per_capita', \n",
    "                   'coal_co2', 'oil_co2', 'gas_co2', 'cement_co2']\n",
    "    if target is None:\n",
    "        target = 'co2_per_capita'\n",
    "\n",
    "    df = df[df['country'] != 'Least developed countries (Jones et al.)']\n",
    "    \n",
    "    groups = df[group_col].unique()\n",
    "    results = []\n",
    "    model_details = []\n",
    "    \n",
    "    for group in groups:\n",
    "        group_df = df[df[group_col] == group].dropna(subset=features + [target])\n",
    "        if len(group_df) < 20:  # Skip groups with insufficient data\n",
    "            continue\n",
    "            \n",
    "        X = group_df[features]\n",
    "        y = group_df[target]\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(random_state=42),\n",
    "            'XGBoost': XGBRegressor(random_state=42)\n",
    "        }\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Store performance results\n",
    "            results.append({\n",
    "                'Group': group,\n",
    "                'Model': name,\n",
    "                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'R2': r2_score(y_test, y_pred),\n",
    "                'Samples': len(group_df)\n",
    "            })\n",
    "            \n",
    "            # Store model coefficients/importance\n",
    "            if name == 'Linear Regression':\n",
    "                coef_data = {\n",
    "                    'Group': group,\n",
    "                    'Model': name,\n",
    "                    'Type': 'Coefficient',\n",
    "                    'Features': features,\n",
    "                    'Values': model.coef_,\n",
    "                    'Intercept': model.intercept_\n",
    "                }\n",
    "            else:  # Tree-based models\n",
    "                coef_data = {\n",
    "                    'Group': group,\n",
    "                    'Model': name,\n",
    "                    'Type': 'Feature Importance',\n",
    "                    'Features': features,\n",
    "                    'Values': model.feature_importances_,\n",
    "                    'Intercept': None\n",
    "                }\n",
    "            model_details.append(coef_data)\n",
    "\n",
    "    # Convert results to DataFrames\n",
    "    results_df = pd.DataFrame(results)\n",
    "    details_df = pd.DataFrame(model_details)\n",
    "    \n",
    "    # Print performance results\n",
    "    print(\"\\n=== Model Performance Results ===\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Print detailed coefficients/importance\n",
    "    print(\"\\n=== Model Coefficients/Feature Importance ===\")\n",
    "    for group in details_df['Group'].unique():\n",
    "        print(f\"\\n--- {group} ---\")\n",
    "        group_details = details_df[details_df['Group'] == group]\n",
    "        \n",
    "        for _, row in group_details.iterrows():\n",
    "            print(f\"\\n{row['Model']} ({row['Type']}):\")\n",
    "            for feature, value in zip(row['Features'], row['Values']):\n",
    "                print(f\"{feature}: {value:.4f}\")\n",
    "            if row['Intercept'] is not None:\n",
    "                print(f\"Intercept: {row['Intercept']:.4f}\")\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=results_df, x='Group', y='R2', hue='Model')\n",
    "    plt.title(f'Model Performance (R²) by {group_col.replace(\"_\", \" \").title()}')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.xlabel(group_col.replace(\"_\", \" \").title())\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df, details_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_bin_results = run_regression_models(df_countries, 'income_bin', ['gdp'], 'co2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sector_trends(df, group_col=None):\n",
    "    sectors = ['coal_co2', 'oil_co2', 'gas_co2', 'cement_co2']\n",
    "    sector_names = [s.replace('_co2', '').replace('_', ' ').title() for s in sectors]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "    df = df[df['year'] >= 1900].copy()  # Filter for years >= 1900\n",
    "    \n",
    "    # Calculate percentage contribution\n",
    "    df['total_co2'] = df[sectors].sum(axis=1)\n",
    "    for sector in sectors:\n",
    "        df[f'{sector}_pct'] = df[sector] / df['total_co2'] * 100\n",
    "    \n",
    "    if group_col is None:\n",
    "        # Global trends\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Calculate mean percentages by year\n",
    "        trend_data = df.groupby('year')[[f'{s}_pct' for s in sectors]].mean()\n",
    "        \n",
    "        # Stacked area plot\n",
    "        plt.stackplot(trend_data.index, trend_data.values.T,\n",
    "                     labels=sector_names, colors=colors, alpha=0.8)\n",
    "        \n",
    "        plt.title('Global CO₂ Emissions Trends by Sector')\n",
    "        plt.ylabel('Percentage Contribution')\n",
    "        plt.xlabel('Year')\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlim(trend_data.index.min(), trend_data.index.max())\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust right margin for legend\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Grouped trends\n",
    "        groups = df[group_col].unique()\n",
    "        n_groups = len(groups)\n",
    "        n_cols = min(3, n_groups)\n",
    "        n_rows = (n_groups + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5*n_rows))\n",
    "        axes = axes.flatten() if n_groups > 1 else [axes]\n",
    "        \n",
    "        for i, group in enumerate(groups):\n",
    "            ax = axes[i]\n",
    "            group_data = df[df[group_col] == group]\n",
    "            trend_data = group_data.groupby('year')[[f'{s}_pct' for s in sectors]].mean()\n",
    "            \n",
    "            ax.stackplot(trend_data.index, trend_data.values.T,\n",
    "                        labels=sector_names, colors=colors, alpha=0.8)\n",
    "            \n",
    "            ax.set_title(f'CO2 Emissions Trends - {group}')\n",
    "            ax.set_ylabel('Percentage Contribution')\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(trend_data.index.min(), trend_data.index.max())\n",
    "        \n",
    "        # Add single legend for all subplots\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='upper right', \n",
    "                  bbox_to_anchor=(0.98, 0.98), title='Sectors')\n",
    "        \n",
    "        # Hide empty subplots if any\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            \n",
    "        plt.tight_layout(rect=[0, 0, 0.95, 1])  # Adjust right margin\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_analysis(df, target_col, feature_cols, group_col=None, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression()\n",
    "    }\n",
    "    \n",
    "    # Remove rows with NaN in target or features\n",
    "    df = df.dropna(subset=[target_col] + feature_cols)\n",
    "\n",
    "    # Preprocessing\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Standardize features (except for tree-based models which don't need it)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Full train-val-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "    print(f\"Train samples: {len(y_train)}, Val samples: {len(y_val)}, Test samples: {len(y_test)}\")\n",
    "    results = []\n",
    "    coefficients = []\n",
    "    \n",
    "    if group_col is None:\n",
    "        # Single dataset analysis\n",
    "        results = _evaluate_models(models, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "        _plot_results(results, X_test, y_test, models)\n",
    "    else:\n",
    "        # Grouped analysis - create faceted plots\n",
    "        groups = df[group_col].unique()\n",
    "        n_groups = len(groups)\n",
    "        n_cols = min(3, n_groups)\n",
    "        n_rows = (n_groups + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
    "        axes = axes.flatten() if n_groups > 1 else [axes]\n",
    "        \n",
    "        for i, group in enumerate(groups):\n",
    "            group_df = df[df[group_col] == group]\n",
    "            X_group = group_df[feature_cols]\n",
    "            y_group = group_df[target_col]\n",
    "            \n",
    "            # Train-test split for group\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                scaler.transform(X_group), y_group, test_size=test_size, random_state=random_state)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "            \n",
    "            # Evaluate models for this group\n",
    "            group_results = _evaluate_models(models, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "            results.extend(group_results)\n",
    "            \n",
    "            # Plot for this group\n",
    "            ax = axes[i]\n",
    "            _plot_group_results(ax, group_results, group)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Print comprehensive metrics\n",
    "    _print_metrics_report(pd.DataFrame(results))\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def _evaluate_models(models, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        val_r2 = r2_score(y_val, y_val_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Val_RMSE': val_rmse,\n",
    "            'Test_RMSE': test_rmse,\n",
    "            'Val_MAE': val_mae,\n",
    "            'Test_MAE': test_mae,\n",
    "            'Val_R2': val_r2,\n",
    "            'Test_R2': test_r2\n",
    "        })\n",
    "    \n",
    "    return results, model.details\n",
    "\n",
    "def _plot_results(results, X_test, y_test, models):\n",
    "    # Metrics comparison\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Model', y='Test_R2', data=metrics_df)\n",
    "    plt.title('Model Comparison by R² Score')\n",
    "    plt.ylabel('R² Score (Test Set)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Actual vs Predicted for best model\n",
    "    best_model_name = metrics_df.loc[metrics_df['Test_R2'].idxmax(), 'Model']\n",
    "    best_model = models[best_model_name]\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Actual vs Predicted - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def _plot_group_results(ax, group_results, group_name):\n",
    "    metrics_df = pd.DataFrame(group_results)\n",
    "    sns.barplot(x='Model', y='Test_R2', data=metrics_df, ax=ax)\n",
    "    ax.set_title(f'{group_name} - Model Comparison')\n",
    "    ax.set_ylabel('R² Score (Test Set)')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "def _print_metrics_report(results_df):\n",
    "    print(\"\\n=== Regression Analysis Report ===\")\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(results_df.groupby('Model')[['Test_RMSE', 'Test_MAE', 'Test_R2']].mean())\n",
    "    \n",
    "    print(\"\\nBest Model by Metric:\")\n",
    "    print(f\"Best R²: {results_df.loc[results_df['Test_R2'].idxmax(), 'Model']}\")\n",
    "    print(f\"Best RMSE: {results_df.loc[results_df['Test_RMSE'].idxmin(), 'Model']}\")\n",
    "    print(f\"Best MAE: {results_df.loc[results_df['Test_MAE'].idxmin(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cfd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_analysis(df, target_col, feature_cols, group_col=None, test_size=0.2, val_size=0.01, random_state=42):\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "    }\n",
    "    \n",
    "    # Remove rows with NaN in target or features\n",
    "    df = df.dropna(subset=[target_col] + feature_cols).copy()\n",
    "    \n",
    "    # Initialize storage\n",
    "    all_results = []\n",
    "    lr_coefficients = []\n",
    "    group_metrics = {}  # To store metrics per group\n",
    "    \n",
    "    # Preprocessing\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    if group_col is None:\n",
    "        # Single dataset analysis\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=test_size, random_state=random_state)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "        \n",
    "        results, coef_data = _evaluate_models(models, X_train, X_val, X_test, y_train, y_val, y_test, feature_cols)\n",
    "        all_results.extend(results)\n",
    "        \n",
    "        if coef_data:\n",
    "            lr_coefficients.append(coef_data)\n",
    "        \n",
    "        _plot_results(results, X_test, y_test, models)\n",
    "    else:\n",
    "        # Grouped analysis\n",
    "        groups = df[group_col].unique()\n",
    "        n_groups = len(groups)\n",
    "        n_cols = min(3, n_groups)\n",
    "        n_rows = (n_groups + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
    "        axes = axes.flatten() if n_groups > 1 else [axes]\n",
    "        \n",
    "        for i, group in enumerate(groups):\n",
    "            group_df = df[df[group_col] == group]\n",
    "            if len(group_df) < 20:  # Skip small groups\n",
    "                print(f\"\\nSkipping group {group} - insufficient data (n={len(group_df)})\")\n",
    "                continue\n",
    "                \n",
    "            X_group = group_df[feature_cols]\n",
    "            y_group = group_df[target_col]\n",
    "            \n",
    "            # Train-test split for group\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                scaler.transform(X_group), y_group, test_size=test_size, random_state=random_state)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "            \n",
    "            # Evaluate models\n",
    "            group_results, coef_data = _evaluate_models(\n",
    "                models, X_train, X_val, X_test, y_train, y_val, y_test, feature_cols)\n",
    "            \n",
    "            if group_results:\n",
    "                all_results.extend(group_results)\n",
    "                group_metrics[group] = group_results  # Store metrics by group\n",
    "                \n",
    "                # Store coefficients\n",
    "                if coef_data:\n",
    "                    coef_data['Group'] = group\n",
    "                    lr_coefficients.append(coef_data)\n",
    "                \n",
    "                # Plot for this group\n",
    "                ax = axes[i]\n",
    "                _plot_group_results(ax, group_results, group)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Print comprehensive metrics report\n",
    "    print(\"\\n=== REGRESSION ANALYSIS REPORT ===\")\n",
    "    \n",
    "    # Print performance metrics per group\n",
    "    if group_col:\n",
    "        print(\"\\n=== PERFORMANCE METRICS BY GROUP ===\")\n",
    "        for group, metrics in group_metrics.items():\n",
    "            print(f\"\\n--- {group} ---\")\n",
    "            metrics_df = pd.DataFrame(metrics)\n",
    "            print(metrics_df[['Model', 'Test_RMSE', 'Test_MAE', 'Test_R2']].to_string(index=False, float_format=\"%.3f\"))\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"\\n=== OVERALL PERFORMANCE ===\")\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    _print_metrics_report(results_df)\n",
    "    \n",
    "    # Print Linear Regression coefficients\n",
    "    if lr_coefficients:\n",
    "        lr_coef_df = pd.DataFrame(lr_coefficients)\n",
    "        print(\"\\n=== LINEAR REGRESSION COEFFICIENTS ===\")\n",
    "        \n",
    "        # Create a clean display of all coefficients\n",
    "        if group_col:\n",
    "            # For grouped analysis\n",
    "            coef_columns = ['Group'] + feature_cols + ['Intercept']\n",
    "            print(lr_coef_df[coef_columns].to_string(float_format=\"%.4f\", index=False))\n",
    "        else:\n",
    "            # For single analysis\n",
    "            coef_columns = feature_cols + ['Intercept']\n",
    "            print(lr_coef_df[coef_columns].to_string(float_format=\"%.4f\", index=False))\n",
    "    \n",
    "    return results_df, lr_coef_df if lr_coefficients else None\n",
    "\n",
    "def _evaluate_models(models, X_train, X_val, X_test, y_train, y_val, y_test, feature_names):\n",
    "\n",
    "    results = []\n",
    "    coef_data = None\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'Model': name,\n",
    "            'Val_RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "            'Test_RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "            'Val_MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "            'Test_MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "            'Val_R2': r2_score(y_val, y_val_pred),\n",
    "            'Test_R2': r2_score(y_test, y_test_pred)\n",
    "        }\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Store coefficients for Linear Regression\n",
    "        if name == 'Linear Regression':\n",
    "            coef_data = {}\n",
    "            # Store all coefficients\n",
    "            for feature, coef in zip(feature_names, model.coef_):\n",
    "                coef_data[feature] = coef\n",
    "            coef_data['Intercept'] = model.intercept_\n",
    "    \n",
    "    return results, coef_data\n",
    "\n",
    "def _print_metrics_report(results_df):\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(results_df.groupby('Model')[['Test_RMSE', 'Test_MAE', 'Test_R2']].mean().to_string(float_format=\"%.3f\"))\n",
    "    \n",
    "    print(\"\\nBest Model by Metric:\")\n",
    "    print(f\"Best R²: {results_df.loc[results_df['Test_R2'].idxmax(), 'Model']} ({results_df['Test_R2'].max():.3f})\")\n",
    "    print(f\"Best RMSE: {results_df.loc[results_df['Test_RMSE'].idxmin(), 'Model']} ({results_df['Test_RMSE'].min():.3f})\")\n",
    "    print(f\"Best MAE: {results_df.loc[results_df['Test_MAE'].idxmin(), 'Model']} ({results_df['Test_MAE'].min():.3f})\")\n",
    "    \n",
    "\n",
    "def _plot_results(results, X_test, y_test, models):\n",
    "    # Metrics comparison\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "\n",
    "    # Actual vs Predicted for best model\n",
    "    best_model_name = metrics_df.loc[metrics_df['Test_R2'].idxmax(), 'Model']\n",
    "    best_model = models[best_model_name]\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Actual vs Predicted - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "# def _plot_group_results(ax, group_results, group_name):\n",
    "#     \"\"\"Plot results for a single group\"\"\"\n",
    "#     metrics_df = pd.DataFrame(group_results)\n",
    "#     sns.barplot(x='Model', y='Test_R2', data=metrics_df, ax=ax)\n",
    "#     ax.set_title(f'{group_name} - Model Comparison')\n",
    "#     ax.set_ylabel('R² Score (Test Set)')\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "def _print_metrics_report(results_df):\n",
    "    print(\"\\n=== Regression Analysis Report ===\")\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(results_df.groupby('Model')[['Test_RMSE', 'Test_MAE', 'Test_R2']].mean())\n",
    "    \n",
    "    print(\"\\nBest Model by Metric:\")\n",
    "    print(f\"Best R²: {results_df.loc[results_df['Test_R2'].idxmax(), 'Model']}\")\n",
    "    print(f\"Best RMSE: {results_df.loc[results_df['Test_RMSE'].idxmin(), 'Model']}\")\n",
    "    print(f\"Best MAE: {results_df.loc[results_df['Test_MAE'].idxmin(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world_per_capita = df_countries[['co2_per_capita', 'energy_per_capita', 'gdp_per_capita','year']].groupby('year').mean().reset_index()\n",
    "df_world_total = df_countries.groupby('year').sum().reset_index()\n",
    "\n",
    "\n",
    "# For grouped analysis (returns coefficients)\n",
    "metrics_df, coef_df = run_regression_analysis(\n",
    "    df_countries,\n",
    "    target_col='co2_per_capita',\n",
    "    feature_cols=['gdp_per_capita'],\n",
    "    random_state=20,\n",
    "    test_size=0.5\n",
    ")\n",
    "\n",
    "# For overall analysis (no coefficients)\n",
    "metrics_df, _ = run_regression_analysis(\n",
    "    df_countries,\n",
    "    target_col='co2_per_capita',\n",
    "    feature_cols=['gdp_per_capita'],\n",
    "    group_col='income_bin',\n",
    "    test_size=0.5,\n",
    "    random_state=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_major = pd.read_csv('GlobalLandTemperaturesByMajorCity.csv', parse_dates=['dt'])\n",
    "df_major = df_major[df_major['dt'].dt.year >= 1950]  \n",
    "\n",
    "\n",
    "print(df_major.isnull().sum())\n",
    "\n",
    "# %% [markdown]\n",
    "# # Getting the best cities with each cluster through the use of k-means \n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "cities = df_major.groupby('City').agg({\n",
    "    'Country': 'first',\n",
    "    'Latitude': 'first',\n",
    "    'Longitude': 'first',\n",
    "    'AverageTemperature': 'count'  \n",
    "}).reset_index()\n",
    "cities.rename(columns={'AverageTemperature': 'DataPoints'}, inplace=True)\n",
    "\n",
    "#ONLY CITIES WITH <80 completeness\n",
    "min_data_points = len(df_major['dt'].unique()) * 0.8\n",
    "qualified_cities = cities[cities['DataPoints'] >= min_data_points].copy()\n",
    "\n",
    "\n",
    "#converter coord to floats\n",
    "def coord_to_float(coord):\n",
    "    if isinstance(coord, str):\n",
    "        try:\n",
    "            value = float(coord[:-1])\n",
    "            direction = coord[-1].upper()\n",
    "            if direction in ['W', 'S']:\n",
    "                return -value\n",
    "            return value\n",
    "        except:\n",
    "            return np.nan\n",
    "    return coord\n",
    "\n",
    "\n",
    "qualified_cities['Latitude'] = qualified_cities['Latitude'].apply(coord_to_float)\n",
    "qualified_cities['Longitude'] = qualified_cities['Longitude'].apply(coord_to_float)\n",
    "\n",
    "#DROP BS LAT OR LONG \n",
    "qualified_cities = qualified_cities.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "\n",
    "\n",
    "def lat_to_float(x):\n",
    "    if isinstance(x, str):\n",
    "        return float(x[:-1]) * (1 if x[-1] == 'N' else -1)\n",
    "    return x\n",
    "qualified_cities['Latitude'] = qualified_cities['Latitude'].apply(lat_to_float)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=30, random_state=42)  \n",
    "\n",
    "qualified_cities['Longitude'] = qualified_cities['Longitude'].apply(coord_to_float)\n",
    "\n",
    "\n",
    "print(qualified_cities[['City', 'Latitude', 'Longitude']].head())\n",
    "\n",
    "#KMEANS\n",
    "kmeans = KMeans(n_clusters=30, random_state=42)\n",
    "qualified_cities['Cluster'] = kmeans.fit_predict(qualified_cities[['Latitude', 'Longitude']])\n",
    "\n",
    "\n",
    "\n",
    "selected_cities = qualified_cities.loc[\n",
    "    qualified_cities.groupby('Cluster')['DataPoints'].idxmax()\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Selected {len(selected_cities)} cities:\")\n",
    "print(selected_cities[['City', 'Country', 'Latitude', 'Longitude']].sort_values('Latitude'))\n",
    "\n",
    "\n",
    "df_selected = df_major[df_major['City'].isin(selected_cities['City'])]\n",
    "print(f\"\\nFiltered dataset shape: {df_selected.shape}\")\n",
    "\n",
    "selected_cities.to_csv('selected_cities.csv', index=False)\n",
    "df_selected.to_csv('filtered_temperatures.csv', index=False)\n",
    "\n",
    "df_major = df_major[df_major['City'].isin(selected_cities['City'])]\n",
    "\n",
    "# %% [markdown]\n",
    "# # Missing data with the use of KNN respects local similarities\n",
    "\n",
    "# %%\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#MISSING KNN DATA INFIL \n",
    "\n",
    "pivot = df_major.pivot_table(index=['dt', 'City'], \n",
    "                      values=['AverageTemperature', 'AverageTemperatureUncertainty'])\n",
    "\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_values = imputer.fit_transform(pivot)\n",
    "\n",
    "\n",
    "df_imputed = (\n",
    "    pd.DataFrame(imputed_values, \n",
    "                columns=pivot.columns, \n",
    "                index=pivot.index)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "#Merge\n",
    "other_cols = df_major.columns.difference(['AverageTemperature', 'AverageTemperatureUncertainty'])\n",
    "df_imputed = df_imputed.merge(\n",
    "    df_major[other_cols].drop_duplicates(['dt', 'City']),\n",
    "    on=['dt', 'City'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "print(df_imputed.head())\n",
    "\n",
    "# %%\n",
    "print(df_imputed.isnull().sum())\n",
    "\n",
    "# %%\n",
    "print(df_imputed.head())\n",
    "\n",
    "# %%\n",
    "df_imputed = df_imputed.round({\n",
    "    'AverageTemperature': 1,       \n",
    "    'AverageTemperatureUncertainty': 1,\n",
    "})\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # QUICK DATA VISIUALIZATION \n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df_major = pd.read_csv('GlobalLandTemperaturesByCity.csv')\n",
    "\n",
    "\n",
    "unique_cities = df_major['City'].nunique()\n",
    "print(f\"Total unique cities in dataset: {unique_cities:,}\")\n",
    "\n",
    "\n",
    "cities_per_country = df_major.groupby('Country')['City'].nunique().sort_values(ascending=False)\n",
    "print(\"\\nCities per country (Top 20):\")\n",
    "print(cities_per_country.head(20))\n",
    "\n",
    "\n",
    "def convert_coord(coord):\n",
    "    if isinstance(coord, str):\n",
    "        value = float(coord[:-1])\n",
    "        return -value if coord[-1] in ['W', 'S'] else value\n",
    "    return coord\n",
    "\n",
    "coord_df = df_major.groupby('City').first()[['Latitude', 'Longitude']].applymap(convert_coord)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "ax.scatter(\n",
    "    coord_df['Longitude'],\n",
    "    coord_df['Latitude'],\n",
    "    c=coord_df.index.map(hash) % 100,  \n",
    "    s=5,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlabel='Longitude',\n",
    "    ylabel='Latitude',\n",
    "    title=f'Global Distribution of {unique_cities:,} Weather Stations'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('city_distribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "full_city_list = df_major[['City', 'Country', 'Latitude', 'Longitude']].drop_duplicates()\n",
    "full_city_list.to_csv('all_cities_list.csv', index=False)\n",
    "\n",
    "print(\"\\nAdditional Stats:\")\n",
    "print(f\"- Cities in Northern Hemisphere: {(coord_df['Latitude'] > 0).sum():,}\")\n",
    "print(f\"- Cities in Southern Hemisphere: {(coord_df['Latitude'] < 0).sum():,}\")\n",
    "print(f\"- Coastal cities (estimated): {(abs(coord_df['Longitude']) < 5).sum():,}\")\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "uuunique_cities = df_imputed['City'].nunique()\n",
    "print(f\"Total unique cities in dataset: {uuunique_cities:,}\")\n",
    "\n",
    "# %%\n",
    "print(df_major['City'].nunique())\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "df_selected = df_imputed[df_imputed['City'].isin(selected_cities['City'])]\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "unique_cities = df_imputed['City'].nunique()\n",
    "print(f\"Total unique cities in dataset: {unique_cities:,}\")\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "selected_cities = pd.read_csv('selected_cities.csv')  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "ax.set_global()\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='azure')\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='dimgray')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray')\n",
    "\n",
    "\n",
    "colors = plt.cm.plasma((selected_cities['Latitude'] + 90) / 180)  \n",
    "scatter = ax.scatter(\n",
    "    selected_cities['Longitude'],\n",
    "    selected_cities['Latitude'],\n",
    "    c=colors,\n",
    "    s=120,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    edgecolors='black',\n",
    "    linewidths=0.5,\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "\n",
    "label_cities = pd.concat([\n",
    "    selected_cities.nlargest(5, 'DataPoints'),\n",
    "    selected_cities.loc[[selected_cities['Latitude'].abs().idxmax()]]  \n",
    "])\n",
    "\n",
    "for _, row in label_cities.iterrows():\n",
    "    ax.text(\n",
    "        row['Longitude'] + 2,\n",
    "        row['Latitude'],\n",
    "        row['City'],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        fontsize=9,\n",
    "        ha='left',\n",
    "        bbox=dict(facecolor='white', alpha=0.7, pad=2, edgecolor='none')\n",
    "    )\n",
    "\n",
    "#CLIMATE zones \n",
    "for lat in [-66.5, -23.5, 23.5, 66.5]:\n",
    "    ax.plot([-180, 180], [lat, lat], \n",
    "            color='red', \n",
    "            linestyle='--', \n",
    "            alpha=0.3, \n",
    "            transform=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(scatter, orientation='horizontal', pad=0.02, aspect=50)\n",
    "cbar.set_label('Latitude (Climate Zones)')\n",
    "cbar.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "cbar.set_ticklabels(['Polar (90°S)', 'Temperate', 'Tropical', 'Temperate', 'Polar (90°N)'])\n",
    "\n",
    "\n",
    "plt.title('Geographic Distribution of Selected 30 Cities\\n'\n",
    "         'Color-coded by Latitude with Climate Zone Boundaries',\n",
    "         pad=20, fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save high-res version\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "print(df_selected.head())\n",
    "\n",
    "# %%\n",
    "\n",
    "# df_imputed = df_imputed.sort_values('dt')\n",
    "\n",
    "\n",
    "# split_idx = int(len(df_imputed) * 0.8)\n",
    "# train = df_imputed.iloc[:split_idx]\n",
    "# test = df_imputed.iloc[split_idx:]\n",
    "\n",
    "# print(f\"Train: {train['dt'].min().date()} to {train['dt'].max().date()}\") \n",
    "# print(f\"Test: {test['dt'].min().date()} to {test['dt'].max().date()}\")\n",
    "# print(f\"Train %: {len(train)/len(df_selected):.1%}, Test %: {len(test)/len(df_imputed):.1%}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # CONVERTING THE COORDS TO FLOATS, N/S/E/W -> FLOATS, CYCLIC ENCODING \n",
    "\n",
    "# %%\n",
    "def convert_coordinate(coord):\n",
    "    if isinstance(coord, str):\n",
    "        direction = coord[-1].upper()  \n",
    "        value = float(coord[:-1])      \n",
    "        if direction in ['S', 'W']:\n",
    "            value *= -1  \n",
    "        return value\n",
    "    return coord  \n",
    "\n",
    "\n",
    "df_imputed['Latitude'] = df_imputed['Latitude'].apply(convert_coordinate)\n",
    "df_imputed['Longitude'] = df_imputed['Longitude'].apply(convert_coordinate)\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "df_imputed['dt'] = pd.to_datetime(df_imputed['dt'])\n",
    "\n",
    "\n",
    "df_imputed['year'] = df_imputed['dt'].dt.year\n",
    "df_imputed['month'] = df_imputed['dt'].dt.month\n",
    "df_imputed['day_of_year'] = df_imputed['dt'].dt.dayofyear\n",
    "\n",
    "\n",
    "df_imputed['month_sin'] = np.sin(2 * np.pi * df_imputed['month'] / 12)\n",
    "df_imputed['month_cos'] = np.cos(2 * np.pi * df_imputed['month'] / 12)\n",
    "\n",
    "\n",
    "city_dummies = pd.get_dummies(df_imputed['City'], prefix='city')\n",
    "df_imputed = pd.concat([df_imputed, city_dummies], axis=1)\n",
    "\n",
    "\n",
    "city_dummy_cols = [col for col in df_imputed.columns if col.startswith('city_')]\n",
    "\n",
    "\n",
    "df_imputed = df_imputed.drop(columns=city_dummy_cols)\n",
    "\n",
    "\n",
    "print(\"Columns after removing one-hot cities:\", df_imputed.columns)\n",
    "\n",
    "\n",
    "# WITHOUT THE CO2 \n",
    "# feature_cols = ['Latitude', 'Longitude', 'year', 'month_sin', 'month_cos'] + \\\n",
    "#                [col for col in df_imputed.columns if col.startswith('city_')]\n",
    "# X = df_imputed[feature_cols]\n",
    "# y = df_imputed['AverageTemperature']\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# %%\n",
    "print(df_imputed)\n",
    "\n",
    "# %%\n",
    "df_emissions = pd.read_csv('owid-co2-data.csv')\n",
    "df_emissions = df_emissions[df_emissions['year'] >= 1950]  # Adjust column name if needed\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # COMBINING THE TWO DATASET AS COUNTRY IS OUR JOINT\n",
    "\n",
    "# %%\n",
    "valid_countries = df_imputed['Country'].unique() \n",
    "df_emissions_filtered = df_emissions[df_emissions['country'].isin(valid_countries)]\n",
    "\n",
    "# %%\n",
    "print(valid_countries)\n",
    "df_emissions_filtered.head()\n",
    "\n",
    "# %%\n",
    "print(\"Columns in emissions dataset:\", df_emissions.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "emission_feat = ['co2', 'co2_per_capita', 'coal_co2', 'country', 'year']\n",
    "df_emissions_filtered = df_emissions[emission_feat].copy()  \n",
    "\n",
    "# %%\n",
    "df_emissions_filtered.head()\n",
    "\n",
    "# %%\n",
    "\n",
    "df_emissions_filtered = df_emissions_filtered.rename(columns={'country': 'Country'})\n",
    "df_emissions_filtered = df_emissions_filtered.rename(columns={'year': 'Year'})\n",
    "\n",
    "df_imputed['Year'] = pd.to_datetime(df_imputed['dt']).dt.year\n",
    "\n",
    "\n",
    "df_combined = pd.merge(\n",
    "    df_imputed,               \n",
    "    df_emissions_filtered,    \n",
    "    on=['Country', 'Year'],   \n",
    "    how='left'                \n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "df_combined['co2'] = df_combined.groupby('Country')['co2'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# %%\n",
    "print(\"Duplicate rows:\", df_combined.duplicated().sum())\n",
    "\n",
    "# %% [markdown]\n",
    "# # FILLING MISSING DATA OF THE CO2 DATASET\n",
    "\n",
    "# %%\n",
    "for col in ['co2', 'co2_per_capita', 'coal_co2']:\n",
    "\n",
    "    country_medians = df_combined.groupby('Country')[col].median()\n",
    "    \n",
    "\n",
    "    df_combined[col] = df_combined[col].fillna(\n",
    "        df_combined['Country'].map(country_medians)\n",
    "    )\n",
    "    \n",
    "\n",
    "    global_median = df_combined[col].median()\n",
    "    df_combined[col] = df_combined[col].fillna(global_median)\n",
    "    \n",
    "print(\"Missing values after imputation:\")\n",
    "print(df_combined[['co2', 'co2_per_capita', 'coal_co2']].isnull().sum())\n",
    "\n",
    "df_combined = df_combined.drop(columns=['Year'], errors='ignore')  \n",
    "\n",
    "# %% [markdown]\n",
    "# # SPLITTING THE DATA INTO TRAINING AND VALIDATION SET 70-30 SPLIT \n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    "    'Latitude', 'Longitude', 'year', 'month_sin', 'month_cos',\n",
    "    'co2', 'co2_per_capita', 'coal_co2'  # Emission features\n",
    "]\n",
    "\n",
    "\n",
    "target_col = 'AverageTemperature'\n",
    "\n",
    "X = df_combined[feature_cols]\n",
    "y = df_combined[target_col]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,  \n",
    "    shuffle=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(\"\\nFeature columns:\", X_train.columns.tolist())\n",
    "\n",
    "# %%\n",
    "\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X_train, y_train)  # Make sure this runs without errors\n",
    "\n",
    "\n",
    "# print(\"Coefficients:\", lr.coef_)\n",
    "# print(\"Intercept:\", lr.intercept_)\n",
    "\n",
    "# %%\n",
    "# print(\"NaN values in X_train:\", X_train.isnull().sum().sum())\n",
    "# print(\"NaN values in y_train:\", y_train.isnull().sum())\n",
    "\n",
    "# %%\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# pipe = make_pipeline(\n",
    "#     SimpleImputer(strategy='median'),  # Handles any remaining NaNs\n",
    "#     LinearRegression()\n",
    "# )\n",
    "\n",
    "# pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# final_model = pipe.named_steps['linearregression']\n",
    "# print(\"Coefficients:\", final_model.coef_)\n",
    "# print(\"Intercept:\", final_model.intercept_)\n",
    "\n",
    "# %% [markdown]\n",
    "# # TESTING THE MODELS PERFORMANCES OF ALL FOUR, BASLEINE LINEAR REGRESSION, RF, NN AND XGBOOST\n",
    "\n",
    "# %%\n",
    "#BASELINE MODEL \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),  # Handles any remaining NaNs\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Train/evaluate\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = lr.predict(X_test)\n",
    "print(f\"Linear Regression:\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"  R²: {r2_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# # Train\n",
    "\n",
    "# rf_optimized = RandomForestRegressor(\n",
    "#     n_estimators=300,\n",
    "#     min_samples_split=10,\n",
    "#     min_samples_leaf=2,\n",
    "#     max_features='log2',\n",
    "#     max_depth=None,  # No limit on tree depth\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1  # Use all CPU cores\n",
    "# )\n",
    "\n",
    "\n",
    "# rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_pred_rf = rf_optimized.predict(X_test)\n",
    "\n",
    "\n",
    "# rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "# r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# print(\"Optimized Random Forest Results:\")\n",
    "# print(f\"- RMSE: {rmse_rf:.4f}\")\n",
    "# print(f\"- R²: {r2_rf:.4f}\")\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric=['rmse', 'mae'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "xgb_r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\nXGBoost Test R²: {xgb_r2:.4f}\")\n",
    "\n",
    "\n",
    "results = xgb_model.evals_result()\n",
    "print(f\"Final Validation RMSE: {results['validation_0']['rmse'][-1]:.4f}\")\n",
    "print(f\"Final Validation MAE: {results['validation_0']['mae'][-1]:.4f}\")\n",
    "\n",
    "# %%\n",
    "print(\"Missing values in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in X_test:\")\n",
    "print(X_test.isnull().sum())\n",
    "\n",
    "# %%\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rf_optimized = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='log2',\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_rf = rf_optimized.predict(X_test)\n",
    "\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "print(\"\\nOptimized Random Forest Results:\")\n",
    "print(f\"- MAE:  {mae_rf:.4f}\")  \n",
    "print(f\"- RMSE: {rmse_rf:.4f}\")\n",
    "print(f\"- R²:   {r2_rf:.4f}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "importances = rf_optimized.feature_importances_\n",
    "features = X_train.columns \n",
    "sorted_idx = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), importances[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), features[sorted_idx])\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=400,\n",
    "    early_stopping_rounds=20,\n",
    "    eval_metric=['rmse', 'mae'],\n",
    "    verbosity=0,\n",
    "    reg_alpha=0.5,  \n",
    "    reg_lambda=1.0, \n",
    "    gamma=0.1,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False  \n",
    ")\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "results = model.evals_result()\n",
    "\n",
    "print(\"\\n=== Final Metrics ===\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {results['validation_1']['rmse'][-1]:.4f}\")  \n",
    "print(f\"MAE: {results['validation_1']['mae'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results['validation_0']['rmse'], label='Train RMSE')\n",
    "plt.plot(results['validation_1']['rmse'], label='Test RMSE')\n",
    "plt.legend()\n",
    "plt.title('RMSE over Training Rounds')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results['validation_0']['mae'], label='Train MAE')\n",
    "plt.plot(results['validation_1']['mae'], label='Test MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE over Training Rounds')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('MAE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "importance = model.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "filtered_importance = {\n",
    "    k: v for k, v in importance.items() \n",
    "    if not k.lower().startswith('city_')\n",
    "}\n",
    "\n",
    "sorted_idx = np.argsort(list(importance.values()))[-20:]  # Top 20 features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    range(len(sorted_idx)),\n",
    "    [list(importance.values())[i] for i in sorted_idx],\n",
    "    align='center'\n",
    ")\n",
    "plt.yticks(\n",
    "    range(len(sorted_idx)),\n",
    "    [list(importance.keys())[i] for i in sorted_idx]\n",
    ")\n",
    "plt.title('XGBoost Feature Importance (Weight)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = rf.predict(X_test)\n",
    "# print(f\"Random Forest:\")\n",
    "# print(f\"  MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "# print(f\"  R²: {r2_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import numpy as np\n",
    "\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 200, 300, 400, 500],\n",
    "#     'max_depth': [None, 10, 20, 30, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': ['sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# rf_search = RandomizedSearchCV(\n",
    "#     estimator=rf,\n",
    "#     param_distributions=param_grid_rf,\n",
    "#     n_iter=50,  # Number of random combinations to try\n",
    "#     cv=5,       # 5-fold cross-validation\n",
    "#     scoring='neg_mean_squared_error',  # Optimize for RMSE\n",
    "#     verbose=1,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1   # Use all CPU cores\n",
    "# )\n",
    "\n",
    "\n",
    "# rf_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# print(\"Best Random Forest Params:\", rf_search.best_params_)\n",
    "\n",
    "# %%\n",
    "# print(\"Best Random Forest parameters:\", rf_search.best_params_)\n",
    "# print(\"Best score (negative MSE):\", rf_search.best_score_)\n",
    "\n",
    "# %%\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# X_train = X_train.astype(np.float32)  \n",
    "# y_train = y_train.astype(np.float32)\n",
    "\n",
    "\n",
    "# model = XGBRegressor(\n",
    "#     objective='reg:squarederror',\n",
    "#     n_estimators=1000,\n",
    "#     early_stopping_rounds=50,\n",
    "#     eval_metric='rmse'\n",
    "# )\n",
    "\n",
    "\n",
    "# model.fit(\n",
    "#     X_train.values, \n",
    "#     y_train.values,\n",
    "#     eval_set=[(X_test.values, y_test.values)],\n",
    "#     verbose=10\n",
    "# )\n",
    "\n",
    "\n",
    "# y_pred = model.predict(X_test.values)\n",
    "# print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),  \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l1_l2(l2=0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']  \n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-5)\n",
    "]\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nNeural Network Results:\")\n",
    "print(f\"- MAE: {mae:.4f}\")  \n",
    "print(f\"- RMSE: {rmse:.4f}\")\n",
    "print(f\"- R²: {r2:.4f}\")\n",
    "print(f\"- Stopped at epoch: {len(history.history['loss'])}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('MAE During Training')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train MSE')\n",
    "plt.plot(history.history['val_loss'], label='Validation MSE')\n",
    "plt.title('MSE Loss During Training')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "models = ['Linear Regression', 'Neural Network', 'Random Forest', 'XGBoost']\n",
    "mae_scores = [5.4900, 0.8246, 0.7856, 0.7469]  \n",
    "r2_scores = [0.3600, 0.9836, 0.9837, 0.9855]\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "\n",
    "bars = ax1.bar(models, mae_scores, color=['#FF9999','#FFB266','#66B2FF','#99FF99'])\n",
    "ax1.set_title('Model Error Comparison', pad=20, fontsize=14)\n",
    "ax1.set_ylabel('Error Metric (MAE/RMSE in original units)', fontsize=12)\n",
    "ax1.set_ylim(0, max(mae_scores)*1.1)\n",
    "\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{height:.2f}',\n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                 xytext=(0, 3),  \n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "\n",
    "bars = ax2.bar(models, r2_scores, color=['#FF9999','#FFB266','#66B2FF','#99FF99'])\n",
    "ax2.set_title('Model Explained Variance (R²)', pad=20, fontsize=14)\n",
    "ax2.set_ylabel('R-squared Score (0-1 scale)', fontsize=12)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{height:.4f}',\n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                 xytext=(0, 3),\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "\n",
    "plt.figtext(0.5, 0.01, \n",
    "            '*Neural Network shows RMSE (1.5694) instead of MAE for comparison', \n",
    "            ha='center', fontsize=10, color='gray')\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # SHAP ANALYSIS MORE IN DEPTH TO HOW AND WHICH FEATURES HAS INFLUENCE \n",
    "\n",
    "# %%\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer(X_train)\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Feature Importance (SHAP Values)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_train, show=False)\n",
    "plt.title(\"Feature Impact on Temperature\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for feature in X_train.columns[:6]:  \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    shap.dependence_plot(\n",
    "        feature, \n",
    "        shap_values.values, \n",
    "        X_train, \n",
    "        interaction_index=None,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"Dependence Plot: {feature}\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sample_idx = 0\n",
    "\n",
    "\n",
    "shap.plots.force(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=shap_values[sample_idx].values,\n",
    "    features=X_train.iloc[sample_idx],\n",
    "    matplotlib=True,\n",
    "    text_rotation=15\n",
    ")\n",
    "\n",
    "\n",
    "features_to_show = [\"Latitude\", \"month_cos\", \"coal_co2\", \"month_sin\", \"Longitude\"]  # Example\n",
    "\n",
    "\n",
    "feature_indices = [X_train.columns.get_loc(f) for f in features_to_show]\n",
    "\n",
    "\n",
    "filtered_shap = shap_values[sample_idx].values[feature_indices]\n",
    "filtered_features = X_train.iloc[sample_idx, feature_indices]\n",
    "\n",
    "\n",
    "shap.plots.force(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=filtered_shap,\n",
    "    features=filtered_features,\n",
    "    feature_names=features_to_show, \n",
    "    matplotlib=True,\n",
    "    text_rotation=15\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
